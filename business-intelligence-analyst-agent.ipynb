{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13598484,"sourceType":"datasetVersion","datasetId":8640827}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.570709Z","iopub.execute_input":"2025-12-01T11:39:49.571480Z","iopub.status.idle":"2025-12-01T11:39:49.584518Z","shell.execute_reply.started":"2025-12-01T11:39:49.571437Z","shell.execute_reply":"2025-12-01T11:39:49.583575Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ecommerce-sales-data-2023-2024/ecommerce_sales_dataset.csv\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"\nBusiness Intelligence Analyst Agent \n\nProblem (Why it matters)\n\nModern businesses generate massive amounts of data from multiple sourcesâ€”sales, marketing, operations, financeâ€”but extracting actionable insights is challenging. Current BI processes often suffer from:\n\nData fragmentation: Data is scattered across different systems (CRM, ERP, web analytics, spreadsheets).\n\nManual reporting: Analysts spend hours cleaning, aggregating, and visualizing data instead of focusing on insights.\n\nSlow decision-making: Managers may not have timely, data-driven insights, leading to delayed or suboptimal decisions.\n\nExample scenario: A retail company wants to identify trends in product sales and customer behavior, but their team struggles to consolidate sales data from online and offline channels, delaying marketing and inventory decisions.\n\nSolution (What youâ€™re proposing)\n\nThe BI Analyst Agent is an AI-driven assistant designed to streamline the full workflow of data analysis. It integrates with multiple data sources, automates data cleaning, performs analysis, and generates actionable reports. Key features include:\n\nAutomated data ingestion & cleaning: Consolidates data from CRM, ERP, spreadsheets, and APIs.\n\nSmart insights generation: Uses BI techniques to uncover trends, anomalies, and predictive insights.\n\nCustom dashboards & reporting: Creates interactive visualizations tailored to business needs.\n\nDecision support: Provides recommendations based on the analysis (e.g., which products to stock, which campaigns to run).\n\nExample scenario: Instead of spending days preparing reports, the retail team receives an AI-generated dashboard showing trending products, seasonal patterns, and predictive sales forecasts in real-time.\n\nValue (Why itâ€™s valuable)\n\nThe BI Analyst Agent delivers measurable business impact by:\n\nSaving time: Reduces manual reporting, freeing analysts to focus on strategic work.\n\nImproving accuracy: Minimizes errors in data consolidation and analysis.\n\nEnabling faster decisions: Provides real-time insights, allowing managers to act quickly.\n\nDriving revenue growth: By identifying trends and opportunities, businesses can optimize operations and marketing.\n\nImpact statement: â€œOur BI Analyst Agent turns raw data into actionable insights automatically, transforming the way businesses make decisions and driving smarter, faster, and more profitable strategies.â€","metadata":{}},{"cell_type":"markdown","source":"****\nCore Concept\n\nThe central idea of this project is an AI-driven Business Intelligence (BI) Analyst Agent that automates the full workflow of data analysis for businesses. Unlike traditional BI tools, which require human intervention for data cleaning, aggregation, and visualization, this agent acts as an autonomous analytical assistant, capable of:\n\nPulling data from multiple sources (CRM, ERP, spreadsheets, APIs).\n\nCleaning, transforming, and integrating disparate datasets.\n\nPerforming advanced analytics (trend analysis, forecasting, anomaly detection).\n\nGenerating actionable insights and recommendations in an easily digestible format.\n\nInnovation: The use of an intelligent agent makes the entire BI process autonomous, context-aware, and adaptive. The agent doesnâ€™t just produce reportsâ€”it interprets them and prioritizes insights, enabling businesses to act proactively instead of reactively.","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\n\ntry:\n    GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n    os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n    print(\"âœ… Gemini API key setup complete.\")\nexcept Exception as e:\n    print(\n        f\"ðŸ”‘ Authentication Error: Please make sure you have added 'GOOGLE_API_KEY' to your Kaggle secrets. Details: {e}\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.586043Z","iopub.execute_input":"2025-12-01T11:39:49.586307Z","iopub.status.idle":"2025-12-01T11:39:49.752537Z","shell.execute_reply.started":"2025-12-01T11:39:49.586286Z","shell.execute_reply":"2025-12-01T11:39:49.751472Z"}},"outputs":[{"name":"stdout","text":"âœ… Gemini API key setup complete.\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"import asyncio\nfrom google.adk.agents import Agent\nfrom google.adk.models.google_llm import Gemini\nfrom google.adk.runners import InMemoryRunner\nfrom google.adk.sessions import InMemorySessionService\nfrom google.adk.tools import google_search\nfrom google.genai import types\nfrom google.adk.tools import AgentTool, FunctionTool, google_search\n\nprint(\"âœ… ADK components imported successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.753564Z","iopub.execute_input":"2025-12-01T11:39:49.754416Z","iopub.status.idle":"2025-12-01T11:39:49.760172Z","shell.execute_reply.started":"2025-12-01T11:39:49.754363Z","shell.execute_reply":"2025-12-01T11:39:49.759240Z"}},"outputs":[{"name":"stdout","text":"âœ… ADK components imported successfully.\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"retry_config = types.HttpRetryOptions(\n    attempts=5,  # Maximum retry attempts\n    exp_base=7,  # Delay multiplier\n    initial_delay=1,\n    http_status_codes=[429, 500, 503, 504],  # Retry on these HTTP errors\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.761104Z","iopub.execute_input":"2025-12-01T11:39:49.761403Z","iopub.status.idle":"2025-12-01T11:39:49.777670Z","shell.execute_reply.started":"2025-12-01T11:39:49.761374Z","shell.execute_reply":"2025-12-01T11:39:49.776818Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"import os\n\nbase_path = '/kaggle/input/'\nprint(os.listdir(base_path))  # lists all datasets added","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.779856Z","iopub.execute_input":"2025-12-01T11:39:49.780125Z","iopub.status.idle":"2025-12-01T11:39:49.791071Z","shell.execute_reply.started":"2025-12-01T11:39:49.780105Z","shell.execute_reply":"2025-12-01T11:39:49.790312Z"}},"outputs":[{"name":"stdout","text":"['ecommerce-sales-data-2023-2024']\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"def clean_data(df):\n    cleaned = df.copy()\n    cleaned.columns = cleaned.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n    cleaned = cleaned.drop_duplicates()\n    cleaned = cleaned.fillna(cleaned.median(numeric_only=True))\n    return cleaned\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.791781Z","iopub.execute_input":"2025-12-01T11:39:49.792030Z","iopub.status.idle":"2025-12-01T11:39:49.803288Z","shell.execute_reply.started":"2025-12-01T11:39:49.792011Z","shell.execute_reply":"2025-12-01T11:39:49.802473Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def generate_eda(df):\n    eda_report = {\n        \"shape\": df.shape,\n        \"columns\": list(df.columns),\n        \"describe\": df.describe().to_dict(),\n        \"nulls\": df.isnull().sum().to_dict(),\n        \"correlation\": df.corr().to_dict()\n    }\n    return eda_report\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.804238Z","iopub.execute_input":"2025-12-01T11:39:49.804603Z","iopub.status.idle":"2025-12-01T11:39:49.817718Z","shell.execute_reply.started":"2025-12-01T11:39:49.804562Z","shell.execute_reply":"2025-12-01T11:39:49.816850Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"def plot_correlation(df):\n    plt.figure(figsize=(10, 6))\n    sns.heatmap(df.corr(), annot=False, cmap='Blues')\n    plt.title(\"Correlation Matrix\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.818668Z","iopub.execute_input":"2025-12-01T11:39:49.819006Z","iopub.status.idle":"2025-12-01T11:39:49.830382Z","shell.execute_reply.started":"2025-12-01T11:39:49.818974Z","shell.execute_reply":"2025-12-01T11:39:49.829598Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import pandas as pd\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n# ---- FIXED PATH ----\ndata = pd.read_csv('/kaggle/input/ecommerce-sales-data-2023-2024/ecommerce_sales_dataset.csv')\n\nprint(\"Dataset Loaded Successfully!\")\nprint(data.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.846994Z","iopub.execute_input":"2025-12-01T11:39:49.847588Z","iopub.status.idle":"2025-12-01T11:39:49.891245Z","shell.execute_reply.started":"2025-12-01T11:39:49.847557Z","shell.execute_reply":"2025-12-01T11:39:49.890258Z"}},"outputs":[{"name":"stdout","text":"Dataset Loaded Successfully!\n  Transaction_ID        Date Customer_ID     Product  Quantity Payment_Method  \\\n0    TXN_0004163  2023-01-01  CUST_00662      Router         6    Credit Card   \n1    TXN_0003251  2023-01-01  CUST_00522  Hard Drive         6    Credit Card   \n2    TXN_0001138  2023-01-01  CUST_00941     Charger         3    Credit Card   \n3    TXN_0002035  2023-01-01  CUST_00799     Speaker         5      Apple Pay   \n4    TXN_0000649  2023-01-01  CUST_00388     Monitor         3     Debit Card   \n\n  Country  Discount_Percentage  Shipping_Cost  Delivery_Days  Customer_Rating  \\\n0   India                    0            5.0           19.0              5.0   \n1     USA                    5           15.0           10.0              5.0   \n2     USA                    0           10.0           29.0              5.0   \n3     USA                    5            5.0            NaN              4.0   \n4     USA                    5           25.0           25.0              NaN   \n\n  Return_Status  Unit_Price  Total_Price_Before_Discount  Discount_Amount  \\\n0            No         247                         1482             0.00   \n1            No         214                         1284            64.20   \n2           Yes         105                          315             0.00   \n3           Yes         189                          945            47.25   \n4            No         466                         1398            69.90   \n\n   Price_After_Discount  Final_Total     Category  \n0               1482.00      1487.00   Networking  \n1               1219.80      1234.80      Storage  \n2                315.00       325.00  Accessories  \n3                897.75       902.75  Accessories  \n4               1328.10      1353.10  Peripherals  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"}],"execution_count":53},{"cell_type":"markdown","source":"Multi-Agent System\n\nIn this project, we implemented a multi-agent system where several specialized agents collaborate to analyze business data. Key agents include:\n\nBI Analyst Agent: Main agent that orchestrates data cleaning, EDA, KPI computation, and insight generation.\n\nVisualization Agent: Generates charts and plots for correlations and trends.\n\nExternal Data Agent: (Optional) Uses Google Search tool to fetch benchmarks or market data.\n\nConcepts Demonstrated:\n\nAgent powered by an LLM: The BI Analyst Agent is driven by Googleâ€™s Gemini LLM to understand natural language queries and orchestrate tasks.\n\nSequential agents: The workflow is sequential â€” first clean the data, then analyze it, then generate visualizations, and finally produce recommendations.\n\nParallel agents (optional): Visualization and KPI computation can run in parallel to optimize performance.","metadata":{}},{"cell_type":"code","source":"from concurrent.futures import ThreadPoolExecutor, as_completed\n\ndata = pd.read_csv('/kaggle/input/ecommerce-sales-data-2023-2024/ecommerce_sales_dataset.csv')\ndef sales_analysis(df):\n    \"\"\"Analyze total sales and top products.\"\"\"\n    total_sales = df['Sales'].sum()\n    top_products = df.groupby('Product')['Sales'].sum().sort_values(ascending=False).head(5)\n    return {\"total_sales\": total_sales, \"top_products\": top_products}\n\ndef customer_segmentation(df):\n    \"\"\"Segment customers by total spending.\"\"\"\n    customer_spending = df.groupby('CustomerID')['Sales'].sum()\n    segments = pd.cut(customer_spending, bins=[0, 100, 500, 1000, 10000], labels=['Low', 'Medium', 'High', 'VIP'])\n    return segments.value_counts()\n\ndef inventory_gap_analysis(df):\n    \"\"\"Find products with low stock.\"\"\"\n    low_stock = df[df['Stock'] < 10].groupby('Product')['Stock'].sum()\n    return low_stock\n\ndef profitability_analysis(df):\n    \"\"\"Calculate profitability per product.\"\"\"\n    df['Profit'] = df['Sales'] - df['Cost']\n    top_profit_products = df.groupby('Product')['Profit'].sum().sort_values(ascending=False).head(5)\n    return top_profit_products\n\n# === 3. Run analyses in parallel ===\nanalysis_functions = [sales_analysis, customer_segmentation, inventory_gap_analysis, profitability_analysis]\n\nresults = {}\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    future_to_task = {executor.submit(func, data): func.__name__ for func in analysis_functions}\n    for future in as_completed(future_to_task):\n        task_name = future_to_task[future]\n        try:\n            results[task_name] = future.result()\n        except Exception as e:\n            results[task_name] = f\"Error: {e}\"\n\n# === 4. Display results ===\nfor task, result in results.items():\n    print(f\"\\n--- {task} ---\")\n    print(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:39:49.892351Z","iopub.execute_input":"2025-12-01T11:39:49.892637Z","iopub.status.idle":"2025-12-01T11:39:49.924055Z","shell.execute_reply.started":"2025-12-01T11:39:49.892600Z","shell.execute_reply":"2025-12-01T11:39:49.923262Z"}},"outputs":[{"name":"stdout","text":"\n--- inventory_gap_analysis ---\nError: 'Stock'\n\n--- customer_segmentation ---\nError: 'CustomerID'\n\n--- sales_analysis ---\nError: 'Sales'\n\n--- profitability_analysis ---\nError: 'Sales'\n","output_type":"stream"}],"execution_count":54},{"cell_type":"markdown","source":"MCP (Multi-Channel Processor)\n\nOrchestrates multiple tasks and tools in parallel\n\nDecides which tool to run based on task and context\n\nCustom Tools\n\nTask-specific tools built for the domain\n\nExamples: sales_analysis, customer_segmentation, inventory_gap, profitability\n\nBuilt-in Tools\n\nPre-existing utilities like Google Search or Code Execution\n\nUseful for dynamic problem-solving or additional computations\n\nOpenAPI Tools\n\nIntegrate external APIs for real-time data, analytics, or reporting\n\nExamples: e-commerce platform metrics, predictive analytics APIs\n\nLong-Running Operations\n\nHandle heavy computations asynchronously\n\nSupport pause/resume, background processing, and checkpointing\n\nKey Idea: Tools work together under MCP orchestration to make the agent autonomous, scalable, and intelligent.","metadata":{}},{"cell_type":"code","source":"import time\nimport threading\nfrom typing import Any, Callable, Dict\n\n\n# ============================================================\n# 1. Base Tool Class  (Define FIRST)\n# ============================================================\nclass Tool:\n    def __init__(self, name: str, func: Callable):\n        self.name = name\n        self.func = func\n\n    def call(self, **kwargs):\n        return self.func(**kwargs)\n\n\n# ============================================================\n# 2. MCP Tool\n# ============================================================\ndef mcp_process(query: str):\n    return {\"tool\": \"MCP\", \"response\": f\"MCP processed: {query}\"}\n\nmcp_tool = Tool(\"mcp\", mcp_process)\n\n\n# ============================================================\n# 3. Custom Tool\n# ============================================================\ndef custom_discount_tool(price: float, discount: float):\n    final_price = price - (price * (discount / 100))\n    return {\"final_price\": final_price}\n\ncustom_tool = Tool(\"discount_tool\", custom_discount_tool)\n\n\n# ============================================================\n# 4. Built-in Tools (Google Search + Code Execution)\n# ============================================================\ndef google_search(query: str):\n    return {\"google_result\": f\"(Simulated search for): {query}\"}\n\ngoogle_tool = Tool(\"google_search\", google_search)\n\n\ndef code_execution(code: str):\n    try:\n        result = eval(code)\n    except Exception as e:\n        result = str(e)\n    return {\"output\": result}\n\ncode_tool = Tool(\"code_execution\", code_execution)\n\n\n# ============================================================\n# 5. OpenAPI Tool (Simulated)\n# ============================================================\ndef openapi_call(endpoint: str, payload: Dict[str, Any]):\n    return {\n        \"endpoint\": endpoint,\n        \"payload\": payload,\n        \"success\": True,\n        \"message\": \"Simulated OpenAPI call finished\"\n    }\n\nopenapi_tool = Tool(\"openapi\", openapi_call)\n\n\n# ============================================================\n# 6. Long-Running Task (Pause / Resume)\n# ============================================================\nclass LongTask:\n    def __init__(self):\n        self.progress = 0\n        self.paused = False\n\n    def run(self):\n        while self.progress < 100:\n            if self.paused:\n                print(\"â¸ï¸ Task paused...\")\n                time.sleep(1)\n                continue\n\n            self.progress += 10\n            print(f\"â³ Progress: {self.progress}%\")\n            time.sleep(1)\n\n        print(\"âœ… Task Completed!\")\n\n    def pause(self):\n        self.paused = True\n\n    def resume(self):\n        print(\"â–¶ï¸ Resuming task...\")\n        self.paused = False\n\n\nlong_task = LongTask()\n\n\n# ============================================================\n# 7. Agent Tool Manager\n# ============================================================\nclass Agent:\n    def __init__(self):\n        self.tools = {\n            \"mcp\": mcp_tool,\n            \"custom\": custom_tool,\n            \"google\": google_tool,\n            \"code\": code_tool,\n            \"openapi\": openapi_tool,\n        }\n\n    def use(self, tool_name: str, **kwargs):\n        if tool_name not in self.tools:\n            return {\"error\": \"Tool not found\"}\n        return self.tools[tool_name].call(**kwargs)\n\n\n# ============================================================\n# 8. Example Usage\n# ============================================================\nagent = Agent()\n\nprint(\"\\nðŸŸ¦ MCP Tool:\")\nprint(agent.use(\"mcp\", query=\"Process ecommerce data\"))\n\nprint(\"\\nðŸŸ§ Custom Tool (Discount):\")\nprint(agent.use(\"custom\", price=200, discount=15))\n\nprint(\"\\nðŸŸ© Google Tool:\")\nprint(agent.use(\"google\", query=\"Top trending products 2024\"))\n\nprint(\"\\nðŸŸ¨ Code Execution Tool:\")\nprint(agent.use(\"code\", code=\"10 * 10 + 5\"))\n\nprint(\"\\nðŸŸª OpenAPI Tool:\")\nprint(agent.use(\"openapi\", endpoint=\"/products\", payload={\"limit\": 5}))\n\n\n# ============================================================\n# 9. Long Task Demo\n# ============================================================\nprint(\"\\nðŸŸ« Starting long-running task...\")\nthread = threading.Thread(target=long_task.run)\nthread.start()\n\ntime.sleep(3)\nlong_task.pause()\n\ntime.sleep(3)\nlong_task.resume()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:43:15.868452Z","iopub.execute_input":"2025-12-01T11:43:15.869480Z","iopub.status.idle":"2025-12-01T11:43:21.889023Z","shell.execute_reply.started":"2025-12-01T11:43:15.869443Z","shell.execute_reply":"2025-12-01T11:43:21.887726Z"}},"outputs":[{"name":"stdout","text":"\nðŸŸ¦ MCP Tool:\n{'tool': 'MCP', 'response': 'MCP processed: Process ecommerce data'}\n\nðŸŸ§ Custom Tool (Discount):\n{'final_price': 170.0}\n\nðŸŸ© Google Tool:\n{'google_result': '(Simulated search for): Top trending products 2024'}\n\nðŸŸ¨ Code Execution Tool:\n{'output': 105}\n\nðŸŸª OpenAPI Tool:\n{'endpoint': '/products', 'payload': {'limit': 5}, 'success': True, 'message': 'Simulated OpenAPI call finished'}\n\nðŸŸ« Starting long-running task...\nâ³ Progress: 10%\nâ³ Progress: 20%\nâ³ Progress: 30%\nâ¸ï¸ Task paused...\nâ¸ï¸ Task paused...\nâ¸ï¸ Task paused...\nâ–¶ï¸ Resuming task...\nâ³ Progress: 40%\nâ³ Progress: 50%\nâ³ Progress: 60%\nâ³ Progress: 70%\nâ³ Progress: 80%\nâ³ Progress: 90%\nâ³ Progress: 100%\nâœ… Task Completed!\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"Concept\n\nSessions and memory enable the BI Analyst Agent to retain context across interactions and over time:\n\nSessions: Temporary context for a specific user interaction or task. Example: while generating a weekly sales report, the agent remembers filters, date ranges, and selected metrics within that session.\n\nMemory: Persistent knowledge stored across multiple sessions. Example: the agent learns a companyâ€™s reporting preferences, commonly used KPIs, or business rules for predictive analytics.\n\nWithout sessions and memory, the agent would treat every interaction independently, requiring the user to repeat context and preferencesâ€”reducing efficiency and personalization.","metadata":{}},{"cell_type":"code","source":"import uuid\nfrom typing import Dict, Any\n\n\n# ============================================================\n# 1. In-Memory Session Service (Short-Term State)\n# ============================================================\nclass InMemorySessionService:\n    def __init__(self):\n        # Stores session data: session_id â†’ {state}\n        self.sessions: Dict[str, Dict[str, Any]] = {}\n\n    def create_session(self):\n        session_id = str(uuid.uuid4())\n        self.sessions[session_id] = {}\n        return session_id\n\n    def set(self, session_id: str, key: str, value: Any):\n        self.sessions[session_id][key] = value\n\n    def get(self, session_id: str, key: str):\n        return self.sessions[session_id].get(key, None)\n\n    def get_all(self, session_id: str):\n        return self.sessions[session_id]\n\n\n# ============================================================\n# 2. Long-Term Memory Bank\n#    Stores permanent knowledge across sessions\n# ============================================================\nclass MemoryBank:\n    def __init__(self):\n        # Global memory persists forever\n        self.long_term_memory: Dict[str, Any] = {}\n\n    def store(self, key: str, value: Any):\n        self.long_term_memory[key] = value\n\n    def recall(self, key: str):\n        return self.long_term_memory.get(key, None)\n\n    def all_memory(self):\n        return self.long_term_memory\n\n\n# ============================================================\n# 3. Agent Using Sessions + Long-Term Memory\n# ============================================================\nclass Agent:\n    def __init__(self, session_service: InMemorySessionService, memory_bank: MemoryBank):\n        self.session_service = session_service\n        self.memory_bank = memory_bank\n\n    def start_session(self):\n        return self.session_service.create_session()\n\n    def remember_in_session(self, session_id: str, key: str, value: Any):\n        self.session_service.set(session_id, key, value)\n        return {\"session_memory_saved\": {key: value}}\n\n    def remember_long_term(self, key: str, value: Any):\n        self.memory_bank.store(key, value)\n        return {\"long_term_memory_saved\": {key: value}}\n\n    def recall_session(self, session_id: str, key: str):\n        return self.session_service.get(session_id, key)\n\n    def recall_long_term(self, key: str):\n        return self.memory_bank.recall(key)\n\n\n# ============================================================\n# 4. Demonstration\n# ============================================================\nsession_service = InMemorySessionService()\nmemory_bank = MemoryBank()\nagent = Agent(session_service, memory_bank)\n\nprint(\"\\n=== Creating Session ===\")\nsession_id = agent.start_session()\nprint(\"New Session ID:\", session_id)\n\nprint(\"\\n=== Saving Short-Term Session Memory ===\")\nagent.remember_in_session(session_id, \"last_query\", \"Show me sales data\")\nagent.remember_in_session(session_id, \"user_name\", \"John Doe\")\n\nprint(\"Session State:\", session_service.get_all(session_id))\n\nprint(\"\\n=== Saving Long-Term Memory ===\")\nagent.remember_long_term(\"favorite_category\", \"Electronics\")\nagent.remember_long_term(\"returning_customer\", True)\n\nprint(\"Long-Term Memory:\", memory_bank.all_memory())\n\nprint(\"\\n=== Recall Short-Term Memory ===\")\nprint(\"Last query:\", agent.recall_session(session_id, \"last_query\"))\n\nprint(\"\\n=== Recall Long-Term Memory ===\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:44:52.448131Z","iopub.execute_input":"2025-12-01T11:44:52.449327Z","iopub.status.idle":"2025-12-01T11:44:52.467133Z","shell.execute_reply.started":"2025-12-01T11:44:52.449285Z","shell.execute_reply":"2025-12-01T11:44:52.465940Z"}},"outputs":[{"name":"stdout","text":"\n=== Creating Session ===\nNew Session ID: af544e6c-bfce-4463-81c4-b581ee453b7f\n\n=== Saving Short-Term Session Memory ===\nSession State: {'last_query': 'Show me sales data', 'user_name': 'John Doe'}\n\n=== Saving Long-Term Memory ===\nLong-Term Memory: {'favorite_category': 'Electronics', 'returning_customer': True}\n\n=== Recall Short-Term Memory ===\nLast query: Show me sales data\n\n=== Recall Long-Term Memory ===\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import uuid\nfrom typing import List, Dict, Any\n\n\n# ============================================================\n# 1. Short-Term Session Memory\n# ============================================================\nclass SessionMemory:\n    def __init__(self):\n        self.sessions: Dict[str, List[Dict[str, str]]] = {}\n\n    def create(self):\n        sid = str(uuid.uuid4())\n        self.sessions[sid] = []\n        return sid\n\n    def add_message(self, session_id: str, role: str, content: str):\n        self.sessions[session_id].append({\"role\": role, \"content\": content})\n\n    def get_history(self, session_id: str):\n        return self.sessions[session_id]\n\n\n# ============================================================\n# 2. Long-Term Memory (facts + user preferences)\n# ============================================================\nclass LongTermMemory:\n    def __init__(self):\n        self.facts: Dict[str, Any] = {}\n\n    def store(self, key: str, value: Any):\n        self.facts[key] = value\n\n    def retrieve(self, key: str):\n        return self.facts.get(key, None)\n\n    def all(self):\n        return self.facts\n\n\n# ============================================================\n# 3. Context Compaction Engine\n# ============================================================\nclass ContextCompactor:\n    def compact(self, messages: List[Dict[str, str]], max_messages: int = 5):\n        \"\"\"\n        Purpose:\n        - Reduce long history.\n        - Keep only key information.\n        \"\"\"\n        if len(messages) <= max_messages:\n            return messages  # no compaction needed\n\n        # Extract the \"core\" of the conversation\n        important_points = []\n        for msg in messages:\n            text = msg[\"content\"].lower()\n\n            # Heuristic rules for what to keep\n            if any(\n                keyword in text\n                for keyword in [\"sales\", \"product\", \"price\", \"dataset\", \"analysis\", \"customer\"]\n            ):\n                important_points.append(msg)\n\n        # If no important messages detected, fallback to newest messages\n        if not important_points:\n            important_points = messages[-max_messages:]\n\n        # Convert to compacted form\n        summary = [\n            {\"role\": \"system\", \"content\": \"Context summary: Key points retained.\"}\n        ] + important_points[-max_messages:]\n\n        return summary\n\n\n# ============================================================\n# 4. Context Builder (sessions + compaction + memory)\n# ============================================================\nclass ContextBuilder:\n    def __init__(self, session_memory: SessionMemory, long_memory: LongTermMemory, compactor: ContextCompactor):\n        self.session_memory = session_memory\n        self.long_memory = long_memory\n        self.compactor = compactor\n\n    def build_context(self, session_id: str):\n        raw_history = self.session_memory.get_history(session_id)\n\n        # compact the history\n        compact_history = self.compactor.compact(raw_history)\n\n        # insert long-term memory facts on top\n        long_memory_data = [\n            {\"role\": \"system\", \"content\": f\"Long-term memory: {key} â†’ {value}\"}\n            for key, value in self.long_memory.all().items()\n        ]\n\n        final_context = long_memory_data + compact_history\n        return final_context\n\n\n# ============================================================\n# 5. Demo Agent\n# ============================================================\nclass Agent:\n    def __init__(self, context_builder: ContextBuilder, session_memory: SessionMemory):\n        self.context_builder = context_builder\n        self.session_memory = session_memory\n\n    def send(self, session_id: str, user_message: str):\n        # save message\n        self.session_memory.add_message(session_id, \"user\", user_message)\n\n        # build context automatically\n        context = self.context_builder.build_context(session_id)\n\n        # (Simulated) agent response\n        response = f\"(Agent using {len(context)} context items) â†’ Got it: {user_message}\"\n\n        # save response\n        self.session_memory.add_message(session_id, \"assistant\", response)\n\n        return response, context\n\n\n# ============================================================\n# 6. Run Example\n# ============================================================\nsession_mem = SessionMemory()\nlong_mem = LongTermMemory()\ncompactor = ContextCompactor()\nbuilder = ContextBuilder(session_mem, long_mem, compactor)\nagent = Agent(builder, session_mem)\n\n# Create session\nsid = session_mem.create()\n\n# Add long-term memory\nlong_mem.store(\"favorite_category\", \"Electronics\")\nlong_mem.store(\"preferred_format\", \"Short answers\")\n\n# Simulate conversation\nagent.send(sid, \"Analyze ecommerce sales dataset\")\nagent.send(sid, \"Which product had the highest revenue last month?\")\nagent.send(sid, \"Show me customer segmentation\")\nagent.send(sid, \"I want to predict future sales using ML\")\nagent.send(sid, \"Plot monthly revenue trend\")\n\n# Now trigger compaction\nresponse, context = agent.send(sid, \"Give me inventory gap analysis\")\n\nprint(\"\\n=== Agent Response ===\")\nprint(response)\n\nprint(\"\\n=== Final Compacted Context ===\")\nfor c in context:\n    print(f\"{c['role'].upper()}: {c['content']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:47:12.826118Z","iopub.execute_input":"2025-12-01T11:47:12.826608Z","iopub.status.idle":"2025-12-01T11:47:12.846745Z","shell.execute_reply.started":"2025-12-01T11:47:12.826575Z","shell.execute_reply":"2025-12-01T11:47:12.845471Z"}},"outputs":[{"name":"stdout","text":"\n=== Agent Response ===\n(Agent using 8 context items) â†’ Got it: Give me inventory gap analysis\n\n=== Final Compacted Context ===\nSYSTEM: Long-term memory: favorite_category â†’ Electronics\nSYSTEM: Long-term memory: preferred_format â†’ Short answers\nSYSTEM: Context summary: Key points retained.\nUSER: Show me customer segmentation\nASSISTANT: (Agent using 7 context items) â†’ Got it: Show me customer segmentation\nUSER: I want to predict future sales using ML\nASSISTANT: (Agent using 8 context items) â†’ Got it: I want to predict future sales using ML\nUSER: Give me inventory gap analysis\n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"import time\nimport uuid\nfrom typing import Any, Dict, List\nimport logging\n\n# ============================================================\n# 1. Setup Logging\n# ============================================================\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(message)s',\n    datefmt='%H:%M:%S'\n)\n\nlogger = logging.getLogger(\"AgentLogger\")\n\n# ============================================================\n# 2. Metrics Collector\n# ============================================================\nclass MetricsCollector:\n    def __init__(self):\n        self.metrics = {\n            \"messages_processed\": 0,\n            \"sessions_created\": 0,\n            \"errors\": 0,\n        }\n\n    def increment(self, key: str, amount: int = 1):\n        if key in self.metrics:\n            self.metrics[key] += amount\n\n    def report(self):\n        return self.metrics.copy()\n\n\n# ============================================================\n# 3. Simple Tracing Decorator\n# ============================================================\ndef trace(func):\n    def wrapper(*args, **kwargs):\n        trace_id = str(uuid.uuid4())[:8]\n        logger.info(f\"[Trace {trace_id}] START {func.__name__}\")\n        start_time = time.time()\n        try:\n            result = func(*args, **kwargs)\n            duration = time.time() - start_time\n            logger.info(f\"[Trace {trace_id}] END {func.__name__} (took {duration:.3f}s)\")\n            return result\n        except Exception as e:\n            duration = time.time() - start_time\n            logger.error(f\"[Trace {trace_id}] ERROR in {func.__name__}: {e} (took {duration:.3f}s)\")\n            raise\n    return wrapper\n\n\n# ============================================================\n# 4. Session & Memory (Simplified)\n# ============================================================\nclass SessionService:\n    def __init__(self):\n        self.sessions: Dict[str, List[str]] = {}\n\n    @trace\n    def create_session(self):\n        session_id = str(uuid.uuid4())\n        self.sessions[session_id] = []\n        return session_id\n\n    @trace\n    def add_message(self, session_id: str, message: str):\n        if session_id in self.sessions:\n            self.sessions[session_id].append(message)\n\n\nclass MemoryBank:\n    def __init__(self):\n        self.memory: Dict[str, Any] = {}\n\n    @trace\n    def store(self, key: str, value: Any):\n        self.memory[key] = value\n\n    @trace\n    def retrieve(self, key: str):\n        return self.memory.get(key)\n\n\n# ============================================================\n# 5. Agent with Observability\n# ============================================================\nclass ObservantAgent:\n    def __init__(self, session_service: SessionService, memory_bank: MemoryBank, metrics: MetricsCollector):\n        self.session_service = session_service\n        self.memory_bank = memory_bank\n        self.metrics = metrics\n\n    @trace\n    def process_message(self, session_id: str, message: str):\n        logger.info(f\"Processing message in session {session_id}: {message}\")\n        self.session_service.add_message(session_id, message)\n        self.metrics.increment(\"messages_processed\")\n\n        # Simulate processing and storing key info in memory\n        if \"favorite\" in message.lower():\n            self.memory_bank.store(\"favorite_item\", message)\n        \n        response = f\"(Agent response) Received: {message}\"\n        return response\n\n\n# ============================================================\n# 6. Demo Run\n# ============================================================\nmetrics = MetricsCollector()\nsessions = SessionService()\nmemory = MemoryBank()\nagent = ObservantAgent(sessions, memory, metrics)\n\n# Create session\nsession_id = sessions.create_session()\nmetrics.increment(\"sessions_created\")\nlogger.info(f\"New session created: {session_id}\")\n\n# Process messages\nmsgs = [\n    \"Analyze ecommerce sales data\",\n    \"User's favorite product is Laptop\",\n    \"Predict future sales trends\"\n]\n\nfor m in msgs:\n    resp = agent.process_message(session_id, m)\n    logger.info(f\"Agent response: {resp}\")\n\n# Report metrics\nlogger.info(f\"Metrics: {metrics.report()}\")\n\n# Access memory\nlogger.info(f\"Memory contents: {memory.memory}\")\n\n# Access session history\nlogger.info(f\"Session history: {sessions.sessions[session_id]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:48:44.282292Z","iopub.execute_input":"2025-12-01T11:48:44.283426Z","iopub.status.idle":"2025-12-01T11:48:44.300103Z","shell.execute_reply.started":"2025-12-01T11:48:44.283383Z","shell.execute_reply":"2025-12-01T11:48:44.299123Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"import logging\nfrom typing import List, Dict\n\n# ============================================================\n# 1. Setup Logging\n# ============================================================\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(message)s',\n    datefmt='%H:%M:%S'\n)\nlogger = logging.getLogger(\"AgentEvaluation\")\n\n# ============================================================\n# 2. Evaluation Metrics\n# ============================================================\nclass EvaluationMetrics:\n    def __init__(self):\n        self.metrics = {\n            \"tasks_attempted\": 0,\n            \"tasks_succeeded\": 0,\n            \"tasks_failed\": 0,\n            \"average_response_quality\": 0.0,\n        }\n        self.response_scores: List[float] = []\n\n    def record_task(self, success: bool, score: float = None):\n        self.metrics[\"tasks_attempted\"] += 1\n        if success:\n            self.metrics[\"tasks_succeeded\"] += 1\n        else:\n            self.metrics[\"tasks_failed\"] += 1\n        if score is not None:\n            self.response_scores.append(score)\n            self.metrics[\"average_response_quality\"] = sum(self.response_scores) / len(self.response_scores)\n\n    def report(self):\n        return self.metrics.copy()\n\n\n# ============================================================\n# 3. Simulated Agent (for demo)\n# ============================================================\nclass DemoAgent:\n    def __init__(self):\n        self.tools = [\"sales_analysis\", \"customer_segmentation\", \"inventory_gap\", \"profitability\"]\n\n    def run_task(self, task_name: str):\n        # Simulate task execution\n        if task_name not in self.tools:\n            return False, 0.0  # failed\n        # Simulate a quality score 0.0-1.0\n        import random\n        success = random.random() > 0.1  # 90% chance of success\n        score = random.uniform(0.7, 1.0) if success else random.uniform(0.0, 0.6)\n        return success, score\n\n\n# ============================================================\n# 4. Agent Evaluator\n# ============================================================\nclass AgentEvaluator:\n    def __init__(self, agent, metrics: EvaluationMetrics):\n        self.agent = agent\n        self.metrics = metrics\n\n    def evaluate_tasks(self, task_list: List[str]):\n        for task in task_list:\n            success, score = self.agent.run_task(task)\n            self.metrics.record_task(success, score)\n            logger.info(f\"Task '{task}' success={success}, score={score:.2f}\")\n\n    def final_report(self):\n        report = self.metrics.report()\n        logger.info(\"=== Agent Evaluation Report ===\")\n        logger.info(report)\n        return report\n\n\n# ============================================================\n# 5. Demo Run\n# ============================================================\nagent = DemoAgent()\nmetrics = EvaluationMetrics()\nevaluator = AgentEvaluator(agent, metrics)\n\ntasks_to_test = [\n    \"sales_analysis\",\n    \"customer_segmentation\",\n    \"inventory_gap\",\n    \"profitability\",\n    \"nonexistent_tool\"  # intentional failure\n]\n\n# Run evaluation\nevaluator.evaluate_tasks(tasks_to_test)\n\n# Report results\nreport = evaluator.final_report()\nprint(report)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:50:27.802547Z","iopub.execute_input":"2025-12-01T11:50:27.803557Z","iopub.status.idle":"2025-12-01T11:50:27.817381Z","shell.execute_reply.started":"2025-12-01T11:50:27.803519Z","shell.execute_reply":"2025-12-01T11:50:27.816486Z"}},"outputs":[{"name":"stdout","text":"{'tasks_attempted': 5, 'tasks_succeeded': 3, 'tasks_failed': 2, 'average_response_quality': 0.6202124761155842}\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"import uuid\nimport queue\nimport threading\nimport time\nfrom typing import Dict, Any\n\n# ============================================================\n# 1. Message Definition\n# ============================================================\nclass A2AMessage:\n    def __init__(self, sender: str, receiver: str, task: str, data: Dict[str, Any]):\n        self.sender = sender\n        self.receiver = receiver\n        self.task = task\n        self.data = data\n        self.message_id = str(uuid.uuid4())\n        self.timestamp = time.time()\n\n    def to_dict(self):\n        return {\n            \"sender\": self.sender,\n            \"receiver\": self.receiver,\n            \"task\": self.task,\n            \"data\": self.data,\n            \"message_id\": self.message_id,\n            \"timestamp\": self.timestamp,\n        }\n\n\n# ============================================================\n# 2. Agent Base Class\n# ============================================================\nclass Agent:\n    def __init__(self, name: str):\n        self.name = name\n        self.inbox = queue.Queue()\n\n    def send(self, receiver: \"Agent\", task: str, data: Dict[str, Any]):\n        msg = A2AMessage(self.name, receiver.name, task, data)\n        receiver.inbox.put(msg)\n        print(f\"[{self.name}] Sent task '{task}' to {receiver.name} (msg_id={msg.message_id})\")\n\n    def process_messages(self):\n        while True:\n            try:\n                msg: A2AMessage = self.inbox.get(timeout=1)\n                print(f\"[{self.name}] Received task '{msg.task}' from {msg.sender}\")\n                self.handle_task(msg)\n            except queue.Empty:\n                break\n\n    def handle_task(self, msg: A2AMessage):\n        # Override this in subclasses\n        print(f\"[{self.name}] Handling task: {msg.task} with data {msg.data}\")\n\n\n# ============================================================\n# 3. Specialized Agent\n# ============================================================\nclass SalesAgent(Agent):\n    def handle_task(self, msg: A2AMessage):\n        if msg.task == \"analyze_sales\":\n            print(f\"[{self.name}] Analyzing sales data: {msg.data}\")\n            # Simulate analysis\n            time.sleep(0.5)\n            print(f\"[{self.name}] Analysis complete for {msg.sender}\")\n        else:\n            print(f\"[{self.name}] Unknown task: {msg.task}\")\n\n\n# ============================================================\n# 4. Demo A2A Communication\n# ============================================================\nagent_a = Agent(\"Agent_A\")\nagent_b = SalesAgent(\"SalesAgent_B\")\n\n# Agent A sends task to B\nagent_a.send(agent_b, task=\"analyze_sales\", data={\"dataset\": \"ecommerce_2023.csv\"})\n\n# Process messages\nthread_b = threading.Thread(target=agent_b.process_messages)\nthread_b.start()\nthread_b.join()\n\nprint(\"\\nâœ… A2A demo complete\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:51:57.801421Z","iopub.execute_input":"2025-12-01T11:51:57.802482Z","iopub.status.idle":"2025-12-01T11:51:59.317623Z","shell.execute_reply.started":"2025-12-01T11:51:57.802444Z","shell.execute_reply":"2025-12-01T11:51:59.316662Z"}},"outputs":[{"name":"stdout","text":"[Agent_A] Sent task 'analyze_sales' to SalesAgent_B (msg_id=2e6cc45b-b832-4efd-b553-28a8b57e4189)\n[SalesAgent_B] Received task 'analyze_sales' from Agent_A\n[SalesAgent_B] Analyzing sales data: {'dataset': 'ecommerce_2023.csv'}\n[SalesAgent_B] Analysis complete for Agent_A\n\nâœ… A2A demo complete\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"from flask import Flask, request, jsonify\nimport uuid\nimport threading\nimport time\nimport logging\n\n# -----------------------------\n# Logging\n# -----------------------------\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\"DeployedAgent\")\n\n# -----------------------------\n# Memory & Sessions\n# -----------------------------\nclass SessionService:\n    def __init__(self):\n        self.sessions = {}\n\n    def create_session(self):\n        session_id = str(uuid.uuid4())\n        self.sessions[session_id] = []\n        return session_id\n\n    def add_message(self, session_id, message):\n        self.sessions[session_id].append(message)\n\n    def get_history(self, session_id):\n        return self.sessions.get(session_id, [])\n\nsession_service = SessionService()\n\n# -----------------------------\n# Simple Agent\n# -----------------------------\nclass Agent:\n    def __init__(self, session_service):\n        self.session_service = session_service\n\n    def respond(self, session_id, user_message):\n        self.session_service.add_message(session_id, {\"role\": \"user\", \"content\": user_message})\n        # Here you could integrate memory, tools, context, A2A calls, etc.\n        response = f\"(Agent response) You said: {user_message}\"\n        self.session_service.add_message(session_id, {\"role\": \"agent\", \"content\": response})\n        return response\n\nagent = Agent(session_service)\n\n# -----------------------------\n# Flask API\n# -----------------------------\napp = Flask(__name__)\n\n@app.route(\"/create_session\", methods=[\"POST\"])\ndef create_session():\n    session_id = session_service.create_session()\n    logger.info(f\"New session created: {session_id}\")\n    return jsonify({\"session_id\": session_id})\n\n@app.route(\"/send_message\", methods=[\"POST\"])\ndef send_message():\n    data = request.json\n    session_id = data.get(\"session_id\")\n    message = data.get(\"message\")\n    if not session_id or not message:\n        return jsonify({\"error\": \"session_id and message are required\"}), 400\n\n    response = agent.respond(session_id, message)\n    history = session_service.get_history(session_id)\n    return jsonify({\"response\": response, \"session_history\": history})\n\n# -----------------------------\n# Run server (in Kaggle use debug=True)\n# -----------------------------\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-01T11:53:47.572469Z","iopub.execute_input":"2025-12-01T11:53:47.573545Z","iopub.status.idle":"2025-12-01T11:53:48.877253Z","shell.execute_reply.started":"2025-12-01T11:53:47.573505Z","shell.execute_reply":"2025-12-01T11:53:48.876010Z"}},"outputs":[{"name":"stdout","text":" * Serving Flask app '__main__'\n * Debug mode: on\n","output_type":"stream"},{"name":"stderr","text":"INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n * Running on all addresses (0.0.0.0)\n * Running on http://127.0.0.1:5000\n * Running on http://172.19.2.2:5000\nINFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\nINFO:werkzeug: * Restarting with watchdog (inotify)\n0.00s - Debugger warning: It seems that frozen modules are being used, which may\n0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n0.00s - to python to disable frozen modules.\n0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\nTraceback (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 991, in launch_instance\n    app.initialize(argv)\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 113, in inner\n    return method(app, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 666, in initialize\n    self.init_sockets()\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 307, in init_sockets\n    self.shell_port = self._bind_socket(self.shell_socket, self.shell_port)\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 244, in _bind_socket\n    return self._try_bind_socket(s, port)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 220, in _try_bind_socket\n    s.bind(\"tcp://%s:%i\" % (self.ip, port))\n  File \"/usr/local/lib/python3.11/dist-packages/zmq/sugar/socket.py\", line 317, in bind\n    super().bind(addr)\n  File \"_zmq.py\", line 917, in zmq.backend.cython._zmq.Socket.bind\n  File \"_zmq.py\", line 179, in zmq.backend.cython._zmq._check_rc\nzmq.error.ZMQError: Address already in use (addr='tcp://127.0.0.1:44879')\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"],"ename":"SystemExit","evalue":"1","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n","output_type":"stream"}],"execution_count":65}]}